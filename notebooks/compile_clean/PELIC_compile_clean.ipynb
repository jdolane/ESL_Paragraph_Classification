{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb848e4",
   "metadata": {},
   "source": [
    "# Compile and Clean the PELIC Dataset\n",
    "<p>This notebook is to load, compile and clean the PELIC dataset.</p>\n",
    "<p>The dataset is found here: <a href=\"https://github.com/ELI-Data-Mining-Group/PELIC-dataset/\">https://github.com/ELI-Data-Mining-Group/PELIC-dataset/</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d1f6dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48a05920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to be using this to process the data later on, so I'll load it now.\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35c5671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "path = '../data/PELIC-dataset/corpus_files/'\n",
    "question = pd.read_csv(path + 'question.csv')\n",
    "answer = pd.read_csv(path + 'answer.csv')\n",
    "student_info = pd.read_csv(path + 'student_information.csv')\n",
    "course = pd.read_csv(path + 'course.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f97e851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames on 'question_id' and 'anon_id'\n",
    "merged_df = pd.merge(answer, question, on='question_id', how='left')\n",
    "merged_df.rename(columns={'stem': 'question'}, inplace=True)\n",
    "merged_df = pd.merge(merged_df, student_info, on='anon_id', how='left')\n",
    "merged_df.rename(columns={'native_language': 'L1'}, inplace=True)\n",
    "merged_df = pd.merge(merged_df, course, on='course_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d865c6",
   "metadata": {},
   "source": [
    "The dataset contains several question types; however, I'm only interested in the paragraphs. That being said, I need to map the question types and filter out the paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3482e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary for question type\n",
    "question_type_mapping = {\n",
    "    1: 'Paragraph writing',\n",
    "    2: 'Short answer',\n",
    "    3: 'Multiple choice',\n",
    "    4: 'Essay',\n",
    "    5: 'Fill-in-the-blank',\n",
    "    6: 'Sentence completion',\n",
    "    7: 'Word bank',\n",
    "    8: 'Chart',\n",
    "    9: 'Word selection',\n",
    "    10: 'Audio recording'\n",
    "}\n",
    "\n",
    "# Create the new 'question_type' column by mapping 'question_type_id' using the mapping dictionary\n",
    "merged_df['question_type'] = merged_df['question_type_id'].map(question_type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81937099",
   "metadata": {},
   "source": [
    "For the purposes of this project, I'm only interested in the student's level, the first language (L1), the question type, the question, and the answer. I'm going to choose these columns for my dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8314111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column names you want to include in the new DataFrame\n",
    "desired_columns = ['level_id','L1','question_type','question','text']\n",
    "# Create the new DataFrame with only the desired columns\n",
    "df = merged_df[desired_columns].copy()\n",
    "# Rename the columns\n",
    "df = df.rename(columns={'level_id': 'level', 'text': 'answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "119f6a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_type\n",
       "Short answer           22808\n",
       "Paragraph writing      17037\n",
       "Essay                   3496\n",
       "Fill-in-the-blank       2036\n",
       "Sentence completion      450\n",
       "Word bank                247\n",
       "Audio recording            1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.question_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c64e34",
   "metadata": {},
   "source": [
    "For the purposes of this project, I'm only interested in paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1726bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include only paragraphs in the dataframe.\n",
    "df = df[df.question_type == \"Paragraph writing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4773c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "664cb848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='int64')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are any answers that contain only empty space\n",
    "df.loc[df['answer'].str.isspace()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2581172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16858 entries, 0 to 46203\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   level          16858 non-null  int64 \n",
      " 1   L1             16858 non-null  object\n",
      " 2   question_type  16858 non-null  object\n",
      " 3   question       16858 non-null  object\n",
      " 4   answer         16858 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 790.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e9e8c",
   "metadata": {},
   "source": [
    "Upon inspection of the answer column, there are still answers that are not full paragraphs (e.g., they only contain one or two words). We're going to include only the paragraphs that contain at least 3 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "672e2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add the number of sentences per text\n",
    "def num_sentences(df):\n",
    "    # Create a copy of the DataFrame to avoid the SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    # Iterate over rows in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Get the answer text from the DataFrame\n",
    "        answer_text = row['answer']\n",
    "        # Process the answer text with spaCy\n",
    "        doc = nlp(answer_text)\n",
    "        # Initialize variables to accumulate total tokens and count of sentences\n",
    "        num_sentences = 0\n",
    "        # Iterate over sentences and accumulate total tokens\n",
    "        for sentence in doc.sents:\n",
    "            num_sentences += 1\n",
    "        # Add num_sentences in the DataFrame\n",
    "        df.loc[index, 'num_sentences'] = num_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5421552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function that adds a column to the df with the number of sentences\n",
    "df = num_sentences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8f93d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove answers that contain only 1 or 2 sentences\n",
    "df = df[(df.num_sentences != 1) & (df.num_sentences != 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e36c29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.,  10.,   5.,   4.,   7.,   6.,   3.,  13.,  11.,  15.,   9.,\n",
       "        17.,  19.,  27.,  29.,  21.,  16.,   8.,  31.,  25.,  32.,  14.,\n",
       "        20.,  18.,  26.,  39.,  30.,  24.,  28.,  23.,  85.,  40.,  22.,\n",
       "        43.,  53.,  36.,  33.,  44.,  34.,  61.,  35.,  38.,  49.,  41.,\n",
       "        46.,  51.,  52.,  48.,  45.,  42., 103.,  65.,  74.,  50.,  37.,\n",
       "        64.,  66.,  78.,  75.,  69.,  71.,  70.,  80.,  67.,  60.,  54.,\n",
       "        57.,  56.,  76.,  47.,  58., 110.,  93.,  73., 104.,  72., 119.,\n",
       "        79.,  98.,  81., 105.,  59.,  77.,  86.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure that there are no answers with only 1 or 2 sentencs\n",
    "df.num_sentences.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "878c6de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_sentences\n",
       "7.0     1322\n",
       "6.0     1322\n",
       "8.0     1267\n",
       "5.0     1165\n",
       "9.0     1126\n",
       "        ... \n",
       "78.0       1\n",
       "69.0       1\n",
       "60.0       1\n",
       "67.0       1\n",
       "86.0       1\n",
       "Name: count, Length: 84, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see how many sentences the majorty of the answers have\n",
    "df.num_sentences.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "042bc695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31736    Two weeks ago, I went to The Mall at Robinson ...\n",
      "18946    1.I've bought an IPOD for 1 month.\\n2.I've liv...\n",
      "41528     When I was a child I grew up with my parents ...\n",
      "4225     I've already hung the balloons, but I haven't ...\n",
      "6478     During the high school, we always shared toget...\n",
      "5682     Writhing5T\\n5/20/07\\nTopic: What is a good par...\n",
      "22979    Get The Knots Out\\nAccording to Heather gilles...\n",
      "17607     George Walker Bush was born in New Haven, Con...\n",
      "24757    First time when my grandmother had a stroke he...\n",
      "43257    On February 20th , 2012, I and my friends made...\n",
      "23494    EX1) Sweden government nurtured free enterpris...\n",
      "37123                               The green hand     ...\n",
      "1191     Dear Lan:\\n\\n Do you remember last time you as...\n",
      "11265    My kitchen is very small space in my house. As...\n",
      "14414    My apartment caught on fire at the night of No...\n",
      "26440     Think about vacation will be exciding about m...\n",
      "25554    Lose an accent could be occured by stopping of...\n",
      "14435    About my hometown\\n My hometown is Kameoka in ...\n",
      "28550    In the November 27, 1940, a Chinese person bir...\n",
      "30613    1- If you are awareof something(non-count),you...\n",
      "38441    I live a small street in pittsburgh.It is a ab...\n",
      "4522     1. profile <2> (n.) \\nDefinition: a short desc...\n",
      "11490    When I woke up the next morning, I was surpris...\n",
      "36952    By the time I came here, I had been in charge ...\n",
      "10232    I miss my dogs.I have seven dogs in Paraguay, ...\n",
      "2517     Reading3F\\nThomas\\n2/1/07\\nrelationship-\\nSent...\n",
      "24800    Climate is not the same as weather happening j...\n",
      "26437    After the semester, I have lots of exciting pl...\n",
      "7139     There are some advices you can do to improve y...\n",
      "44786     3 months ago I went through the worst experie...\n",
      "44993    Because of liquor could be had easily at a loc...\n",
      "23030    I will tell you about myth of Korean Originati...\n",
      "1409     About 4 days ago, ANON_NAME_0 was play soccer ...\n",
      "948      A writer once said that a good man finds all t...\n",
      "19392     Studying in the U.S. has a lot of advantages....\n",
      "39756     If I had been born in the USA, I would be a n...\n",
      "25238     talking about a friend who consider as a best...\n",
      "38692    Having had a promotion at work, my father earn...\n",
      "30962     Spring Festival\\nI had never forget the day m...\n",
      "36625    The authors of \"The Creative Spirit\" believe t...\n",
      "34036    1. You shouldn't call your boos by his first n...\n",
      "32362    Title of Article: A vote to fire all teachers ...\n",
      "1015     Oct. 16, 2006\\nDear mom,\\n\\n Recently, I notic...\n",
      "17736    At the end of the 19th century, there was one ...\n",
      "30689    I come from Taiwan. Taiwan is in the subtropic...\n",
      "2275     Negotiation is important thing to me as a cons...\n",
      "35553     disadvantages live in small town\\n Living in ...\n",
      "23012    Topic 8: Why junk food manufactures should or ...\n",
      "18729    In some countries, teenagers have jobs while t...\n",
      "14390    The Simple Life Goes to Camp.\\n\\nThe summery:\\...\n",
      "Name: answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get a random sample of answers just to see what they look like\n",
    "print(df.answer.sample(n=50, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "730661db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13849 entries, 0 to 46203\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   level          13849 non-null  int64  \n",
      " 1   L1             13849 non-null  object \n",
      " 2   question_type  13849 non-null  object \n",
      " 3   question       13849 non-null  object \n",
      " 4   answer         13849 non-null  object \n",
      " 5   num_sentences  13849 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 757.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check to see how much data we have left\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01b9a2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "4    6072\n",
       "5    3928\n",
       "3    3750\n",
       "2      99\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need the dataset to be balanced. Let's see how much data we have for each level\n",
    "df.level.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4800941f",
   "metadata": {},
   "source": [
    "There's not a lot of level 2 (pre-intermediate). We're going to have to try to augment that. However, I'm going to save my current df to a csv, and augment the data in a different notebook, since the num_sentences function takes a little while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c6a1ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/PELIC_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
